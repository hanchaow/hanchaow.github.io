<?xml version="1.0" encoding="utf-8"?>
<search>
    
    
    <entry>
        <title><![CDATA[Caffe之SoftmaxWithLossLayer原理推导和代码解析]]></title>
        <url>http://yoursite.com/2017/10/20/Caffe_SoftmaxLossLayer/</url>
        <content type="html"><![CDATA[<h1 id="Caffe之-SoftmaxWithLossLayer-原理推导和代码解析"><a href="#Caffe之-SoftmaxWithLossLayer-原理推导和代码解析" class="headerlink" title="Caffe之 SoftmaxWithLossLayer 原理推导和代码解析"></a>Caffe之 SoftmaxWithLossLayer 原理推导和代码解析</h1><p>前言：我们都知道在使用Caffe训练分类模型时，经常使用的 <code>Loss</code>是 <code>SoftmaxWithLoss</code>, 其具体实现是caffe的 <code>SoftmaxWithLossLayer</code> 类，源代码参考文件：</p>
<ul>
<li>softmax_layer.h、softmax_layer.cpp</li>
<li>softmax_loss_layer.h、softmax_loss_layer.cpp</li>
</ul>
<p>那么，究竟什么是SoftmaxWithLoss，其原理是什么，Caffe中又是如何实现的？本文将逐一揭开SoftmaxWithLoss的神秘面纱……</p>
<h2 id="SoftmaxWithLoss"><a href="#SoftmaxWithLoss" class="headerlink" title="SoftmaxWithLoss"></a>SoftmaxWithLoss</h2><p>SoftmaxWithLoss可以用如下公式表示：</p>
<script type="math/tex; mode=display">
SoftmaxWithLoss \quad = \quad Softmax  \quad + \quad Cross-Entropy Loss \\</script><p>可以看出，SoftmaxWithLoss包含两部分：</p>
<ul>
<li>Softmax：将一串数字转化为一个概率分布</li>
<li>Cross-Entropy Loss：交叉熵Loss函数</li>
</ul>
<h3 id="SoftmaxWithLoss-之-Softmax"><a href="#SoftmaxWithLoss-之-Softmax" class="headerlink" title="SoftmaxWithLoss 之 Softmax"></a>SoftmaxWithLoss 之 Softmax</h3><p>我们先简单介绍下Softmax。</p>
<p>上面提到Softmax就是将一串数字转化为一个概率分布，直观地去理解就是，在做分类模型时，给定一个输入（或者样本），给出一串概率值，其中每个值分别代表着属于某一类的概率；理想的情况是每个样本属于其真实类别的概率值为1，而属于其他类别的概率值为0。</p>
<p>Softmax从直观上给出了某个样本属于某个类别的概率值，其具体的公式如下：</p>
<p>$假设输入一串数字：\quad  input \quad = \quad [a_1, a_2, a_3, …, a_i,…,a_n] \$</p>
<p>$做两个操作：$</p>
<script type="math/tex; mode=display">
\left\{
\begin{aligned}
\;  & a_i & = & \quad  a_i - max(a_1, a_2, a_3, ..., a_i,...,a_n)  & \qquad(1)\\
\;  & f(a_i)  & = & \quad \frac{e^{a_i}}{\sum_{1}^{n} e^{a_j}}  & \qquad(2)\\

\end{aligned}
\right.</script><p>上述两步变换就是Softmax所做的具体（全部）事情：将输入 $a_i$ 变成了一个概率值 $p_i = f(a_i) =\frac{e^{a_i}}{\sum_{1}^{n} e^{a_j}}$</p>
<h3 id="SoftmaxWithLoss-之-Cross-Entropy"><a href="#SoftmaxWithLoss-之-Cross-Entropy" class="headerlink" title="SoftmaxWithLoss 之 Cross-Entropy"></a>SoftmaxWithLoss 之 Cross-Entropy</h3><p>信息熵是信息学领域由香浓提出来的，反映着信息的不确定性，具体公式是：</p>
<script type="math/tex; mode=display">
H \; = \; -\sum_{i=1}^{n} \; p(x_i) \dot \; log \; p(x_i)</script><p>类似地，对于分类问题，假设已知样本在每个类别上的概率分布和其真实label，我们定义交叉熵损失（代价）函数：</p>
<script type="math/tex; mode=display">
Loss \; = \; -\sum_{i=1}^{n} \; y_i \; \cdot \; log \; p_i  \quad = \quad -log \; p_i\\</script><p>其中 $y_i$ ：</p>
<script type="math/tex; mode=display">
\left\{
\begin{aligned}
\; & y_i & = & \quad1 \quad ，\quad当i是样本的真实label时\\
\; & y_i & = &\quad 0 \quad ，\quad当i不是样本的真实label时\\
\end{aligned}
\right.</script><p>$而，p_i:是样本属于真实标签  i  的概率值$</p>
<h3 id="Softmax-Cross-Entropy"><a href="#Softmax-Cross-Entropy" class="headerlink" title="Softmax + Cross-Entropy"></a>Softmax + Cross-Entropy</h3><p>把两个串起来，就可以得到 SoftmaxWithLoss 公式：</p>
<script type="math/tex; mode=display">
\begin{aligned}
Loss \quad & = \quad -log \; p_i \\
& = \quad -log \; f(a_i)  \\
& = \quad -log \; \frac{e^{a_i}}{\sum_{1}^{n} e^{a_j}}\\
& = \quad -a_i + log \;\sum_{1}^{n} e^{a_j}
\end{aligned}</script><p>接下来，我们继续推导 $Loss$ 对任意输入 $a_k$ 的偏微分：$\frac{\partial L}{\partial a_k}$ :</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial L}{\partial a_k} \quad & =  \frac{\partial }{\partial a_k} \;(-a_i + log \;\sum_{1}^{n} e^{a_j}) \\
& = \frac{\partial (-a_i)}{\partial a_k} \; + \frac{\partial (log \; \sum_{1}^{n} e^{a_j})}{\partial a_k}\\
& = \frac{\partial (-a_i)}{\partial a_k} \; + \frac{1}{ \sum_{1}^{n} e^{a_j}} \;\cdot \frac{\partial ( \sum_{1}^{n} e^{a_j})}{\partial a_k}\\
& = \frac{\partial (-a_i)}{\partial a_k} \; + \frac{1}{ \sum_{1}^{n} e^{a_j}} \;\cdot \frac{\partial }{\partial a_k} \;(e^{a_1} + e^{a_2} + ... + e^{a_k} + ... + e^{a_n}) \\

& = \frac{\partial (-a_i)}{\partial a_k} \; + \frac{1}{ \sum_{1}^{n} e^{a_j}} \;\cdot \frac{\partial }{\partial a_k} \;(e^{a_k}) \\
& = \frac{\partial (-a_i)}{\partial a_k} \; + \frac{e^{a_k}}{\sum_{1}^{n} e^{a_j}}\\
& = \frac{\partial (-a_i)}{\partial a_k} \; + f(a_k)\\
\end{aligned}</script><p>$分两种情况：\quad k \;=\; i  \quad和\quad k \; != \; i$</p>
<script type="math/tex; mode=display">
\left\{
\begin{aligned}
\frac{\partial L}{\partial a_k} \quad & = \quad -1 + f(a_k) & = & \quad p_i -1  & ( & k \;=\; i \;即k是样本的真实label) \\
\frac{\partial L}{\partial a_k} \quad & = \quad 0 + f(a_k)  & = & \quad p_i & ( &k \; != \; i \;即k不是样本的真实labe)\\
\end{aligned}
\right.</script><p>Ok，SoftamxWithLoss的基本原理大致如此，等后期会陆续解释为什么要用Softmax，以及为什么要用Cross-Entropy代价函数。考虑到本篇文章的主题，这里不再接受，接下来我们看下Caffe中的源码是如何具体实现的。</p>
<h2 id="Caffe源码之-SoftamxWithLossLayer"><a href="#Caffe源码之-SoftamxWithLossLayer" class="headerlink" title="Caffe源码之 SoftamxWithLossLayer"></a>Caffe源码之 SoftamxWithLossLayer</h2><p>Caffe中的 SoftamxWithLossLayer 实现文件是在 include和src \layers目录下的softmax_loss_layer.h和softmax_loss_layer.cpp，由于Caffe中已经有SoftamaxLayer，所以为了代码（模块）复用，在实现 SoftamxWithLossLayer时直接使用 SoftamaxLayer 这个类，整体来讲：</p>
<script type="math/tex; mode=display">
SoftamxWithLossLayer \quad = \quad SoftamaxLayer的 Forward() \quad +  \quad 全部的Backward</script><p>接下来，我们按照下面的条目把 SoftamxWithLossLayer 拆开讲解：</p>
<ul>
<li>Layer的 bottom 、top 以及关键变量介绍</li>
<li>Layer的 LayerSetUp()方法</li>
<li>Layer的 Reshape()方法</li>
<li>Layer的 Forward_cpu()方法</li>
<li>Layer的 Backward_cpu()方法</li>
</ul>
<h3 id="SoftamxWithLossLayer-之输入输出和关键变量"><a href="#SoftamxWithLossLayer-之输入输出和关键变量" class="headerlink" title="SoftamxWithLossLayer 之输入输出和关键变量"></a>SoftamxWithLossLayer 之输入输出和关键变量</h3><p><em>1.</em> SoftamxWithLossLayer 之输入：<strong>bottom</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//声明</span></div><div class="line"><span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom</div><div class="line"></div><div class="line"><span class="comment">//backward部分代码</span></div><div class="line">Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();</div><div class="line"><span class="keyword">const</span> Dtype* prob_data = prob_.cpu_data();</div><div class="line">caffe_copy(prob_.count(), prob_data, bottom_diff);</div><div class="line"><span class="keyword">const</span> Dtype* label = bottom[<span class="number">1</span>]-&gt;cpu_data();</div></pre></td></tr></table></figure>
<p>bottom 类型是Blob的一个vector，作为 SoftamxWithLossLayer的输入，包括两个元素：</p>
<ul>
<li>bottom[0]，这个blob 包括 data 和 diff 两块存储空间（详细请参考blob的源码）：<ul>
<li>data 是 Layer 前向传播的输入，即理论所讲的 $一串数字（一般是全连接的输出）：\quad [a_1, a_2, a_3, …, a_i,…,a_n]$ ，注意，每个样本都有一串数字，所以data的大小一般是 N*C，N 是样本数，C 是输入的元素数目即 n</li>
<li>diff 是 Layer 反向传播的输出，用于将Loss的梯度向后传播，其值是理论所讲的 $\frac{\partial L}{\partial a_k}$</li>
</ul>
</li>
<li>bottom[1]，这个blob的 data 存储的是每个样本的真实label，其大小是 N，同样N是样本数</li>
</ul>
<p><em>2.</em> SoftamxWithLossLayer 之输出：<strong>top</strong><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//声明</span></div><div class="line"><span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top</div><div class="line"></div><div class="line"><span class="comment">//forward部分代码</span></div><div class="line">top[<span class="number">0</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] = loss / get_normalizer(normalization_, count);</div><div class="line"> <span class="keyword">if</span> (top.size() == <span class="number">2</span>) &#123;</div><div class="line">   top[<span class="number">1</span>]-&gt;ShareData(prob_);</div><div class="line"> &#125;</div></pre></td></tr></table></figure></p>
<p>top 的类型同样是是Bolob的一个vector，作为 SoftamxWithLossLayer 的输出：</p>
<ul>
<li>如果top的 size = 1，那么 top[0] 只存储 SoftamxWithLossLayer的输出（data域），即loss值，对应于理论所讲的 $-log\;p_i$</li>
<li>如果top的 size = 2， 那么 top[1]还会存储 SoftamxWith 预测的概率值prob_ （data域），即 $p_i$</li>
</ul>
<p><em>3.</em> SoftamxWithLossLayer 之 <strong>关键变量</strong></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/// The internal SoftmaxLayer used to map predictions to a distribution.</span></div><div class="line"><span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt; softmax_layer_;</div><div class="line"><span class="comment">/// prob stores the output probability predictions from the SoftmaxLayer.</span></div><div class="line">Blob&lt;Dtype&gt; prob_;  </div><div class="line"><span class="comment">/// bottom vector holder used in call to the underlying SoftmaxLayer::Forward</span></div><div class="line"><span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; softmax_bottom_vec_;</div><div class="line"><span class="comment">/// top vector holder used in call to the underlying SoftmaxLayer::Forward</span></div><div class="line"><span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt; softmax_top_vec_;</div><div class="line"><span class="comment">/// Whether to ignore instances with a certain label.</span></div><div class="line"><span class="keyword">bool</span> has_ignore_label_;</div><div class="line"><span class="comment">/// The label indicating that an instance should be ignored.</span></div><div class="line"><span class="keyword">int</span> ignore_label_;</div><div class="line"><span class="comment">/// How to normalize the output loss.</span></div><div class="line">LossParameter_NormalizationMode normalization_;</div><div class="line"></div><div class="line"><span class="keyword">int</span> softmax_axis_, outer_num_, inner_num_;</div></pre></td></tr></table></figure>
<ul>
<li>softmax_layer_ ： SoftmaxLayer ，用于将预测结果（全连接的输出）映射为一个概率分布</li>
<li>prob_ ：SoftmaxLayer的输出，即预测的概率值</li>
<li>softmax_bottom_vec_ ： SoftmaxLayer 的 bottom</li>
<li>softmax_top_vec_ : SoftmaxLayer 的 top</li>
<li>has_ignore_label_ ： 是否有label需要忽略</li>
<li>ignore_label_ ：要忽略的 label</li>
<li>normalization_ : loss的归一化方式</li>
<li>softmax_axis_ ： 哪一个维度要做softmax，一般是是 C (Caffe中的blob结构一般是 N<em>C</em>H*W)</li>
<li>outer_num_ : 一般是 N</li>
<li>inner_num_ : 一般是 H*W</li>
</ul>
<p><em>4.</em> SoftamxWithLossLayer 之 <strong>LayerSetUp</strong></p>
<p>LayerSetUp 函数简单总结如下：</p>
<ul>
<li>创建一个 SoftmaxLayer ，并设置其 bottom 和 top<ul>
<li>SoftmaxLayer的 bottom 即 SoftmaxWithLossLayer的 bottom</li>
<li>SoftmaxLayer的 top 存放的是 prob_，即概率值输出</li>
</ul>
</li>
<li>对忽略标签和loss归一化的相关设置</li>
</ul>
<p>具体代码如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SoftmaxWithLossLayer&lt;Dtype&gt;::LayerSetUp(</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  LossLayer&lt;Dtype&gt;::LayerSetUp(bottom, top);</div><div class="line">  <span class="function">LayerParameter <span class="title">softmax_param</span><span class="params">(<span class="keyword">this</span>-&gt;layer_param_)</span></span>;</div><div class="line">  <span class="comment">// 1、创建一个 SoftmaxLayer</span></div><div class="line">  softmax_param.set_type(<span class="string">"Softmax"</span>);</div><div class="line">  softmax_layer_ = LayerRegistry&lt;Dtype&gt;::CreateLayer(softmax_param);</div><div class="line">  softmax_bottom_vec_.clear();</div><div class="line">  <span class="comment">// 2、初始化 SoftmaxLayer 的bottom和top ，可以看到：</span></div><div class="line">  <span class="comment">// SoftmaxLayer的 bottom 即 SoftmaxWithLossLayer的 bottom</span></div><div class="line">  <span class="comment">// SoftmaxLayer的 top 存放的是 prob_，即概率值输出</span></div><div class="line">  softmax_bottom_vec_.push_back(bottom[<span class="number">0</span>]);</div><div class="line">  softmax_top_vec_.clear();</div><div class="line">  softmax_top_vec_.push_back(&amp;prob_);</div><div class="line">  softmax_layer_-&gt;SetUp(softmax_bottom_vec_, softmax_top_vec_);</div><div class="line"></div><div class="line">  <span class="comment">//3、对忽略标签和loss归一化的相关设置</span></div><div class="line">  has_ignore_label_ =</div><div class="line">    <span class="keyword">this</span>-&gt;layer_param_.loss_param().has_ignore_label();</div><div class="line">  <span class="keyword">if</span> (has_ignore_label_) &#123;</div><div class="line">    ignore_label_ = <span class="keyword">this</span>-&gt;layer_param_.loss_param().ignore_label();</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (!<span class="keyword">this</span>-&gt;layer_param_.loss_param().has_normalization() &amp;&amp;</div><div class="line">      <span class="keyword">this</span>-&gt;layer_param_.loss_param().has_normalize()) &#123;</div><div class="line">    normalization_ = <span class="keyword">this</span>-&gt;layer_param_.loss_param().normalize() ?</div><div class="line">                     LossParameter_NormalizationMode_VALID :</div><div class="line">                     LossParameter_NormalizationMode_BATCH_SIZE;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    normalization_ = <span class="keyword">this</span>-&gt;layer_param_.loss_param().normalization();</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><em>4.</em> SoftamxWithLossLayer 之 <strong>Reshape</strong></p>
<p>Reshape 函数简单总结如下：</p>
<p>（1）reshpe LossLayer 和 softmax_layer_</p>
<p>（2）读取要做 softmax的维度（支持负索引，比如-1），一般是 1，并求出 outer_num_ 和 inner_num_</p>
<p>（3）做一些检查： bottom[1]的大小是 N，N个样本，每个样本对应一个label，总的label数是C（{0, 1, …, C-1}）</p>
<p>（4）如果的top的size &gt;=2 ，那么 top[1] 存放prob值即softmax的输出</p>
<p>具体代码如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SoftmaxWithLossLayer&lt;Dtype&gt;::Reshape(</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  <span class="comment">//1、reshpe LossLayer</span></div><div class="line">  LossLayer&lt;Dtype&gt;::Reshape(bottom, top);</div><div class="line">  <span class="comment">//2、reshape softmax_layer_</span></div><div class="line">  softmax_layer_-&gt;Reshape(softmax_bottom_vec_, softmax_top_vec_);</div><div class="line">  <span class="comment">//3、读取要做 softmax的维度（支持负索引，比如-1），一般是 1</span></div><div class="line">  softmax_axis_ =</div><div class="line">      bottom[<span class="number">0</span>]-&gt;CanonicalAxisIndex(<span class="keyword">this</span>-&gt;layer_param_.softmax_param().axis());</div><div class="line">  outer_num_ = bottom[<span class="number">0</span>]-&gt;count(<span class="number">0</span>, softmax_axis_);  <span class="comment">// N ，样本数</span></div><div class="line">  inner_num_ = bottom[<span class="number">0</span>]-&gt;count(softmax_axis_ + <span class="number">1</span>); <span class="comment">// H*W， 一般是1</span></div><div class="line">  <span class="comment">// 4、 bottom[1]的大小是 N，N个样本，每个样本对应一个label，总的label数是C（&#123;0, 1, ..., C-1&#125;）</span></div><div class="line">  CHECK_EQ(outer_num_ * inner_num_, bottom[<span class="number">1</span>]-&gt;count())</div><div class="line">      &lt;&lt; <span class="string">"Number of labels must match number of predictions; "</span></div><div class="line">      &lt;&lt; <span class="string">"e.g., if softmax axis == 1 and prediction shape is (N, C, H, W), "</span></div><div class="line">      &lt;&lt; <span class="string">"label count (number of labels) must be N*H*W, "</span></div><div class="line">      &lt;&lt; <span class="string">"with integer values in &#123;0, 1, ..., C-1&#125;."</span>;</div><div class="line">  <span class="comment">//5、如果的top的size &gt;=2 ，那么 top[1] 存放prob值即softmax的输出</span></div><div class="line">  <span class="keyword">if</span> (top.size() &gt;= <span class="number">2</span>) &#123;</div><div class="line">    <span class="comment">// softmax output</span></div><div class="line">    top[<span class="number">1</span>]-&gt;ReshapeLike(*bottom[<span class="number">0</span>]);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><em>5.</em> SoftamxWithLossLayer 之 <strong>Forward_cpu</strong></p>
<p>Forward_cpu 函数简单总结如下：</p>
<p>（1） softmax_layer_ 前向传播求出 prob_</p>
<p>（2）计算所有样本的 loss， $loss \;-=\; log(\; max(prob, \; Dtype(FLT_MIN)) \;)$</p>
<p>注意，做了一个最小的溢出保护，理论部分是 $loss  = -log(p)$</p>
<p>（3）根据所有样本的 loss 和 归一化 方式，求出最终的top</p>
<p>具体代码如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SoftmaxWithLossLayer&lt;Dtype&gt;::Forward_cpu(</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</div><div class="line">  <span class="comment">// The forward pass computes the softmax prob values.</span></div><div class="line">  <span class="comment">// 1、softmax_layer_ 前向传播求出 prob</span></div><div class="line">  softmax_layer_-&gt;Forward(softmax_bottom_vec_, softmax_top_vec_);</div><div class="line">  <span class="keyword">const</span> Dtype* prob_data = prob_.cpu_data();</div><div class="line">  <span class="keyword">const</span> Dtype* label = bottom[<span class="number">1</span>]-&gt;cpu_data();</div><div class="line">  <span class="keyword">int</span> dim = prob_.count() / outer_num_;</div><div class="line">  <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">  Dtype loss = <span class="number">0</span>;</div><div class="line"> <span class="comment">// 2、计算所有样本的 loss， loss -= log( max(prob, Dtype(FLT_MIN)) )</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; outer_num_; ++i) &#123;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; inner_num_; j++) &#123;</div><div class="line">      <span class="keyword">const</span> <span class="keyword">int</span> label_value = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(label[i * inner_num_ + j]);</div><div class="line">      <span class="keyword">if</span> (has_ignore_label_ &amp;&amp; label_value == ignore_label_) &#123;</div><div class="line">        <span class="keyword">continue</span>;</div><div class="line">      &#125;</div><div class="line">      DCHECK_GE(label_value, <span class="number">0</span>);</div><div class="line">      DCHECK_LT(label_value, prob_.shape(softmax_axis_));</div><div class="line">      <span class="comment">//loss公式</span></div><div class="line">      loss -= <span class="built_in">log</span>(<span class="built_in">std</span>::max(prob_data[i * dim + label_value * inner_num_ + j],</div><div class="line">                           Dtype(FLT_MIN)));</div><div class="line">      ++count;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// 3、根据总的 loss 和 归一化 方式，求出最终的top</span></div><div class="line">  top[<span class="number">0</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] = loss / get_normalizer(normalization_, count);</div><div class="line">  <span class="keyword">if</span> (top.size() == <span class="number">2</span>) &#123;</div><div class="line">    top[<span class="number">1</span>]-&gt;ShareData(prob_);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><em>6.</em> SoftamxWithLossLayer 之 <strong>Backward_cpu</strong></p>
<p>Backward_cpu 主要是求Loss对输入的梯度，回归前面的理论部分：</p>
<p>$对于任意输入 a_k，分两种情况：\quad k \;=\; i  \quad和\quad k \; != \; i$</p>
<script type="math/tex; mode=display">
\left\{
\begin{aligned}
\frac{\partial L}{\partial a_k} \quad & = \quad -1 + f(a_k) & = & \quad p_k -1  & ( & k \;=\; i \;即k是样本的真实label) \\
\frac{\partial L}{\partial a_k} \quad & = \quad 0 + f(a_k)  & = & \quad p_k & ( &k \; != \; i \;即k不是样本的真实labe)\\
\end{aligned}
\right.</script><p>（1）反向传播求 bottom_diff</p>
<ul>
<li>初始 bottom_diff 初始值为 prob_，即softmax的输出</li>
<li>用公式表示：$\frac{\partial L}{\partial a_k} = p_k$</li>
</ul>
<p>（2）对于忽略的label, bottom_diff = 0</p>
<p>（3）对于真实label的 $a_k$ , bottom_diff 要减去1</p>
<p>具体代码如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SoftmaxWithLossLayer&lt;Dtype&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,</div><div class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123;</div><div class="line">  <span class="comment">// propagate_down 指示bottom中那个blob需要做 backward ，我们知道 bottom[0]是输入，需要做 backward ，而</span></div><div class="line">  <span class="comment">// bttom[1] 是 label，不能做backward，所以这里做了个检查，像EuclideanLossLayer则没有这种要求，</span></div><div class="line">  <span class="comment">// 但是代码判断起来比较复杂， 需要根据 propagate_down来判断bottom中哪个是输入 哪个是label值</span></div><div class="line">  <span class="keyword">if</span> (propagate_down[<span class="number">1</span>]) &#123;</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="keyword">this</span>-&gt;type()</div><div class="line">               &lt;&lt; <span class="string">" Layer cannot backpropagate to label inputs."</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">//反向传播，求 bottom_diff</span></div><div class="line">  <span class="keyword">if</span> (propagate_down[<span class="number">0</span>]) &#123;</div><div class="line">    Dtype* bottom_diff = bottom[<span class="number">0</span>]-&gt;mutable_cpu_diff();</div><div class="line">    <span class="keyword">const</span> Dtype* prob_data = prob_.cpu_data();</div><div class="line">    <span class="comment">// 1、bottom_diff 初始值为 prob_，即softmax的输出</span></div><div class="line">    caffe_copy(prob_.count(), prob_data, bottom_diff);</div><div class="line">    <span class="keyword">const</span> Dtype* label = bottom[<span class="number">1</span>]-&gt;cpu_data();</div><div class="line">    <span class="keyword">int</span> dim = prob_.count() / outer_num_;</div><div class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; outer_num_; ++i) &#123;</div><div class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; inner_num_; ++j) &#123;</div><div class="line">        <span class="keyword">const</span> <span class="keyword">int</span> label_value = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(label[i * inner_num_ + j]);</div><div class="line">        <span class="keyword">if</span> (has_ignore_label_ &amp;&amp; label_value == ignore_label_) &#123;</div><div class="line">          <span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; bottom[<span class="number">0</span>]-&gt;shape(softmax_axis_); ++c) &#123;</div><div class="line">            <span class="comment">//2、对于忽略的label, bottom_diff = 0</span></div><div class="line">            bottom_diff[i * dim + c * inner_num_ + j] = <span class="number">0</span>;</div><div class="line">          &#125;</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">          <span class="comment">//3、 对于真实label的 a_k , bottom_diff要减去1</span></div><div class="line">          bottom_diff[i * dim + label_value * inner_num_ + j] -= <span class="number">1</span>;</div><div class="line">          ++count;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// Scale gradient</span></div><div class="line">    Dtype loss_weight = top[<span class="number">0</span>]-&gt;cpu_diff()[<span class="number">0</span>] /</div><div class="line">                        get_normalizer(normalization_, count);</div><div class="line">    caffe_scal(prob_.count(), loss_weight, bottom_diff);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
        
        <categories>
            
            <category> 深度学习 </category>
            
            <category> Caffe </category>
            
            <category> LossLayers </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Caffe </tag>
            
            <tag> Loss </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[Logistic回归分析]]></title>
        <url>http://yoursite.com/2017/09/07/logistic/</url>
        <content type="html"><![CDATA[<h1 id="Logistic回归分析"><a href="#Logistic回归分析" class="headerlink" title="Logistic回归分析"></a>Logistic回归分析</h1><p>$\qquad Logistic$回归为概率型非线性回归模型，机器学习常用的二类分类器，其表达式为：</p>
<script type="math/tex; mode=display">
\begin{align*}
&z=w_{1}*x_{1}+w_{2}*x_{2}+\cdots+w_{n}*x_{n}+b=\sum_{i=0}^n w_{i}x_{i}  （其中 b等于w_{0}，x_{0}等于1）\\
&f(x) = \frac{1}{1+exp(-z)}
\end{align*}</script><p>$\qquad$即对于二分类，如果$f(x)\ge{0.5}$,则$x$属于第一类，即预测$y=1$，反之$x$属于第二类，预测$y=0$；样本的分布如下，其中，$C_1$表示第一个类别，$C_2$表示第二个类别，样本个数为$n$：</p>
<script type="math/tex; mode=display">
\begin{align*}
&trainingData &x^1 \quad x^2 \quad x^3 \quad \cdots \quad x^n   \\
&labels &C_1 \quad C_1 \quad C_2 \quad \cdots \quad C_1
\end{align*}</script><p>$\qquad$我们的目的是：对于类别为$1$的正样本$f_{w,b}(x)$ 尽可能大,而类别为$2$的负样本$f_{w,b}(x)$ 尽可能小,则我们需要最大化：$L(w,b)=f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))\cdots\ f_{w,b}(x^n)$<br>来寻找最佳的$w$和$b$，即：</p>
<script type="math/tex; mode=display">
w^*,b^* = arg\max\limits_{w,b}(L(w,b))\Longrightarrow\ w^*,b^* = arg\min\limits_{w,b}(-ln{L(w,b)})</script><h1 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a><a href="https://zh.wikipedia.org/zh-hans/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95" target="_blank" rel="external">随机梯度下降法</a></h1><p>$\qquad$ 我们需要优化的函数：</p>
<script type="math/tex; mode=display">
\begin{align*}
-ln{L(w,b)} = -\{ln{f_{w,b}(x^1)}+lnf_{w,b}(x^2)+ln(1-f_{w,b}(x^3))+\cdots lnf_{w,b}(x^n)\}\quad \\
\end{align*}</script><script type="math/tex; mode=display">
\qquad 假设：
\begin{cases}
\hat{y} = 1 \qquad \qquad x\in1 \\\
\\
\hat{y} = 0 \qquad \qquad x\in0
\end{cases}
\qquad 已知\,f(x) = \frac{1}{1+exp(-z)}\quad z = \sum_{i=0}^n  w_{i}x_{i} \qquad 则</script><p>$\qquad$ 我们需要优化的函数简化为：</p>
<script type="math/tex; mode=display">
\begin{align*}
&ln{L(w,b)} =\sum_{j=1}^{n}\{\hat{y}^j\,lnf_{w,b}(x^j)+(1-\hat{y}^j)\,ln(1-f_{w,b}(x^j))\} \\
&当\,\,\hat{y}=1时\quad \hat{y}\,lnf_{w,b}(x)+(1-\hat y)\,ln(1-f_{w,b}(x)) = lnf_{w,b}(x)  \\
&当\,\,\hat{y}=0时\quad \hat{y}\,lnf_{w,b}(x)+(1-\hat y)\,ln(1-f_{w,b}(x)) = ln(1-f_{w,b}(x))
\end{align*}</script><p>$\qquad$ 即均满足上式 , 因此有:</p>
<script type="math/tex; mode=display">
\begin{align*}
\frac{\partial lnL(w,b)}{\partial w_i}=\sum_{j=1}^{n}\hat{y}^j\frac{ \partial lnf_{w,b}(x^j) }{\partial w_i}+(1-\hat{y}^j)\frac{\partial (1-lnf_{w,b}(x^j))}{\partial w_i} \\
\end{align*}</script><p>$\qquad$ 而：</p>
<script type="math/tex; mode=display">
\begin{align*}
\frac{\partial lnf_{w,b}(x)}{\partial w_i}
&=\frac{\partial lnf_{w,b}(x)}{\partial z}*\frac{\partial z}{\partial w_i} \\
&=\frac{1}{f_{w,b}(x)}* \frac{\partial f_{w,b}(x)}{\partial z}*x_i \\
&=\frac{1}{f_{w,b}(x)}*f_{w,b}(x)*(1-f_{w,b}(x))*x_i \\
&=(1-f_{w,b}(x))*x_i \\
\end{align*}</script><p>$\qquad$ 同理，有：</p>
<script type="math/tex; mode=display">\frac{\partial (1-lnf_{w,b}(x))}{\partial w_i}=f_{w,b}(x)*x_i</script><p>$\qquad$ 则化简后有：</p>
<script type="math/tex; mode=display">
\begin{align*}
\frac{\partial lnL(w,b)}{\partial w_i}
&=\sum_{j=1}^{n}\hat{y}^j\frac{ \partial lnf_{w,b}(x^j) }{\partial w_i}+(1-\hat{y}^j)\frac{\partial (1-lnf_{w,b}(x^j))}{\partial w_i} \\
&=\sum_{j=1}^{n}\{\hat{y}^j(1-f_{w,b}(x^j))x^j_i+(1-\hat{y}^j)*f_{w,b}(x^j)x^j_i\} \\
&= \sum_{j=1}^{n}(\hat{y}^j -f_{w,b}(x^j))x^j_i
\end{align*}</script><p>$\qquad b的推导与w的相似，可以得到w的更新迭代过程：w_{i} \leftarrow w_{i}-\alpha*\sum_{j=0}^{n}(\hat{y}^j-f_{w,b}(x^j))x^j_i$</p>
<p>$\qquad$简单总结对比下logistic回归和线性回归：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>$\quad \, Step$</th>
<th>$\qquad \qquad \qquad logistic \quad regression$</th>
<th>$\quad  \qquad  linear \quad regression$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\quad functions$</td>
<td>$\qquad \qquad\quad f_{w,b}(x) = \delta(\sum_{i=0}^{n}w_ix_i+b)$</td>
<td>$\qquad \quad f_{w,b}(x) = \sum_{i=0}^{n}w_ix_i+b$</td>
</tr>
<tr>
<td>$loss function$</td>
<td>$\qquad \qquad\qquad\hat{y}:1 \,\, for\,\, C_1,0\,\, for \,\, C_2 $    $\qquad \quad ln{L(w,b)} =\sum_{j=1}^{n}\{\hat{y}^j\,lnf_{w,b}(x^j)+(1-\hat{y}^j)\,ln(1-f_{w,b}(x^j))\}$</td>
<td>$\qquad\quad \hat{y} : a \quad real\quad number $ $\qquad \quad L(w,b) =\frac{1}{2} \sum_{j=1}^{n}(f_{w,b}(x^j)-\hat{y}^j)^2$</td>
</tr>
<tr>
<td>$optimize$</td>
<td>$\qquad \qquad\quad w_{i} \leftarrow w_{i}-\alpha*\sum_{j=0}^{n}(\hat{y}^j-f_{w,b}(x^j))x^j_i$</td>
<td>$\qquad\quad w_{i} \leftarrow w_{i}-\alpha*\sum_{j=0}^{n}(\hat{y}^j-f_{w,b}(x^j))x^j_i$</td>
</tr>
</tbody>
</table>
</div>
<h1 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h1><h2 id="为什么选用-crossEntropy-损失函数，而不用L2损失函数"><a href="#为什么选用-crossEntropy-损失函数，而不用L2损失函数" class="headerlink" title="为什么选用$crossEntropy$损失函数，而不用L2损失函数"></a>为什么选用$crossEntropy$损失函数，而不用L2损失函数</h2><p>$\qquad答:logistic不像linear \,\, regression使用L2损失函数的原因，主要是由于logistic的funcion的形式，由于sigmoid函数的存在，如果logistic采取L2 loss时，损失函数为：$</p>
<p>$\qquad \qquad \frac{\partial (f_{w,b}(x)-\hat{y})^2}{\partial w_i}=2(f_{w,b}(x)-\hat{y})f_{w,b}(x)(1-f_{w,b}(x))x_i$</p>
<p>$\qquad 则当\hat{y}=1, f_{w,b}(x) = 1 \quad 预测为1 ，即预测完全正确时 \quad loss=0 \quad$</p>
<p>$\qquad 但是当\hat{y}=1,f_{w,b}(x) = 0 \quad 预测为0 ，即预测完全错误时 \quad loss却依然为0 \quad显然不对$</p>
<h2 id="logistic-regression-的分类概率为什么选取了-sigmoid-函数"><a href="#logistic-regression-的分类概率为什么选取了-sigmoid-函数" class="headerlink" title="$logistic \,\,regression$的分类概率为什么选取了$sigmoid$函数"></a><a href="https://www.zhihu.com/question/54707359" target="_blank" rel="external">$logistic \,\,regression$的分类概率为什么选取了$sigmoid$函数</a></h2><p>$\qquad 答: 我们假设样本的分布服从二次高斯分布，即$</p>
<p>$\qquad f_{\mu,\Sigma}(x) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp\{-\frac{1}{2}(x-\mu)^T|\Sigma|^{-1}(x-\mu)\},其中\mu为均值，\Sigma为协方差矩阵$</p>
<p>$\qquad 输入为x，输出f_{\mu,\Sigma}(x)为样本x的概率密度，高斯分布的形状分布取决于均值\mu和协方差矩阵\Sigma,因此需要求取最佳的高斯分布来满足样本的分布$</p>
<script type="math/tex; mode=display">
\begin{align*}
&Maximum Likelihood : L(\mu,\Sigma) = f_{\mu,\Sigma}(x^1)f_{\mu,\Sigma}(x^2)f_{\mu,\Sigma}(x^3)\cdots\cdots\ f_{\mu,\Sigma}(x^{N}) \\
&\mu^*,\Sigma^* = arg\max\limits_{\mu,\Sigma}L(\mu,\Sigma) \\
&\mu^* = \frac{1}{N}\sum_{i=0}^{N}{x^i} \\
&\Sigma^* = \frac{1}{N}\sum_{i=0}^{N}{(x^i-\mu^*)(x^i-\mu^*)^T}
\end{align*}</script><p>$\qquad对于一个二分类，我们假设类别1的样本高斯分布的均值为\mu^1,类别2的样本的高斯分布均值为\mu^2,他们具有相同的协方差\Sigma$</p>
<script type="math/tex; mode=display">
\begin{align*}
&\mu^1 = \sum_{i=1}^{n_1} x_i\quad (x_i \in C_1) ,\qquad \mu^2 = \sum_{i=1}^{n_2} x_i\quad(x_i \in C_2) \\
&\Sigma^1 = \sum_{i=1}^{n_1}(x_i-u^1)(x_i-u^1)^T ,\qquad \Sigma^2 = \sum_{i=1}^{n_2}(x_i-u^2)(x_i-u^2)^T \\
&\Sigma=\frac{n_1}{n_1+n_2}\Sigma^1+\frac{n_1}{n_1+n_2}\Sigma^2
\end{align*}</script><p>$\qquad对于样本x，如果属于C_1则有：$</p>
<script type="math/tex; mode=display">
\begin{align*}
$P(C_{1}|x)
&= \frac{P(C_{1},x)}{P(x)} \\
&=\frac{P(x|C_{1})*P(C_{1})}{P(x|C_{1})*P(C_{1})+P(x|C_{2})*P(C_{2})} \\
&=\frac{1}{1+\frac{P(x|C_{2})*P(C_{2})}{P(x|C_{1})*P(C_{1})}} \\
&=\frac{1}{1+exp(-\alpha)}

\qquad 其中\,\, \alpha= \ln(\frac{P(x|C_{1})*P(C_{1})}{P(x|C_{2})*P(C_{2})})
\end{align*}</script><p>$\qquad将P(x|C_i)带入高斯分布的公式:$</p>
<script type="math/tex; mode=display">
\begin{align*}
&P(x|C_1) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp\{-\frac{1}{2}(x-\mu^1)^T|\Sigma|^{-1}(x-\mu^1)\} \\
&P(x|C_2) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp\{-\frac{1}{2}(x-\mu^2)^T|\Sigma|^{-1}(x-\mu^2)\} \\
&P(C_1)=\frac{n_1}{n_1+n_2}\\
&P(C_2)=\frac{n_2}{n_1+n_2} \\
\end{align*}</script><p>$\qquad其中\alpha, w, b 分别为:$</p>
<script type="math/tex; mode=display">
\begin{align*}
\alpha
&= lnP(x|C_1)-lnP(x|C_2)+ln\frac{P(C_1)}{P(C_2)} \\
&=-\frac{1}{2}(x-\mu^1)^T|\Sigma|^{-1}(x-\mu^1)-(-\frac{1}{2}(x-\mu^2)^T|\Sigma|^{-1}(x-\mu^2))+ln\frac{n_1}{n_2}\\
&=-\frac{1}{2}x^T(\Sigma)^{-1}x+(u^1)^T(\Sigma)^{-1}x-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}x^T(\Sigma)^{-1}x-(u^2)^T(\Sigma)^{-1}x+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\
&=(u^1-u^2)^T(\Sigma)^{-1}x-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\
&= wx+b \\

\\
&w = (u^1-u^2)^T(\Sigma)^{-1} \quad \\
&b =-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2} \\
\end{align*}</script><p>$\qquad 因此可以得到对于满足猜想的二次高斯分布的datasets，生成模型的分类表达式与logistic是一致的$</p>
<h1 id="生成model与判别model对比"><a href="#生成model与判别model对比" class="headerlink" title="生成model与判别model对比"></a>生成model与判别model对比</h1><h2 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h2><p>$\qquad$ 基于现有的样本，对样本分布做了一个猜测（极大似然），因此当数据集较少，或者有噪声的时候，都能达到一个较好的结果(不过分依赖于实际样本),并且可以根据不同的概率model完成样本分布的gauss</p>
<h2 id="判别模型"><a href="#判别模型" class="headerlink" title="判别模型"></a>判别模型</h2><p>$\qquad$ 基于决策的方式（判别式），通过优化方法(sgd)寻找最优参数，对样本的依赖大，样本充足时，其效果一般比生成模型好(基于事实 not 基于猜测)</p>
<h1 id="小扩展"><a href="#小扩展" class="headerlink" title="小扩展"></a>小扩展</h1><h2 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h2><p>$\qquad$ 基于先验概率得出的每个类别的后验概率为softmax函数，即：</p>
<script type="math/tex; mode=display">
\begin{align*}
P(C_i|x)
&= \frac{P(x|C_i)P(C_i)}{\sum_{j=1}^{n}P(x|C_j)P(C_j)}\\
&= \frac{exp(a_k)}{\sum_{j=1}^{n}a_j}
\end{align*}</script><h2 id="待续"><a href="#待续" class="headerlink" title="待续"></a>待续</h2>]]></content>
        
        <categories>
            
            <category> 机器学习 </category>
            
            <category> 经典算法 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Logistic </tag>
            
            <tag> 回归分析 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[C/C++读写ini配置文件]]></title>
        <url>http://yoursite.com/2017/06/30/program-language-c-cplus/</url>
        <content type="html"><![CDATA[<h1 id="问题由来"><a href="#问题由来" class="headerlink" title="问题由来"></a>问题由来</h1><p>最近使用C++写的一个小程序，输入是人脸框、头肩框的位置，根据人脸框和头肩框的位置实现人的跟踪，即输出是不同人脸框的跟踪号，为了保证跟踪号不丢、不重复、不交叉互换，设置了很多策略和阀值，考虑到阀值种类太多，为了测试出比较好的阀值，在实际测试过程中需要反复设置不同的阀值，为此，考虑使用ini配置文件的方式进行设置</p>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>第一次使用C++解析ini配置文件，这里首选使用微软在Windows下提供的api，毕竟站在巨人的肩膀上更容易成功（当然也可以自己造轮子实现一个ini文件的解析器）</p>
<h2 id="头文件"><a href="#头文件" class="headerlink" title="头文件"></a>头文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">#include &lt;Winbase.h&gt;  //VC中默认是包含该文件的</div></pre></td></tr></table></figure>
<h2 id="API预览"><a href="#API预览" class="headerlink" title="API预览"></a>API预览</h2><p>Winbase.h 中提供了很多API函数来对ini配置文件进行读写，这里只介绍用的三个：</p>
<ul>
<li><code>GetPrivateProfileInt</code>       从私有初始化文件获取整型数值</li>
<li><code>GetPrivateProfileString</code>  从私有初始化文件获取字符串型值</li>
<li><code>WritePrivateProfileString</code> 写字符串到私有初始化文件</li>
</ul>
<h2 id="函数原型声明："><a href="#函数原型声明：" class="headerlink" title="函数原型声明："></a>函数原型声明：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">UINT GetPrivateProfileInt(LPCTSTR lpAppName,LPCTSTR lpKeyName,INT nDefault,LPCTSTR lpFileName);</div><div class="line">DWORD GetPrivateProfileString(LPCTSTR lpAppName,LPCTSTR lpKeyName,LPCTSTR lpDefaut,LPSTR lpReturnedString,DWORD nSize,LPCTSTR lpFileName);//读取.ini文件</div><div class="line">bool WritePrivateProfileString(LPCTSTR lpAppName,LPCTSTR lpKeyName,LPCTSTR lpString,LPCTSTR lpFileName);</div></pre></td></tr></table></figure>
<p>相关的参数解释：</p>
<ul>
<li>LPCTSTR lpAppName ： INI文件中的一个字段名</li>
<li>LPCTSTR lpKeyName ： lpAppName 下的一个键名，也就是里面具体的变量名</li>
<li>LPCTSTR lpString  ： 是键值，也就是变量的值， 必须为LPCTSTR或CString类型</li>
<li>LPCTSTR lpFileName ：完整的INI文件路径名</li>
<li>LPCTSTR lpDefaut ：如果没有其前两个参数值，则将此值赋给变量</li>
<li>LPSTR lpReturnedString ： 接收INI文件中的值的CString对象，即接收缓冲区</li>
<li>DWORD nSize ：接收缓冲区的大小</li>
</ul>
<h2 id="使用例子"><a href="#使用例子" class="headerlink" title="使用例子"></a>使用例子</h2><ol>
<li>获取执行程序的路径</li>
<li>获取执行程序的目录（通过查找\在字符串中最后一次出现的位置）</li>
<li>组合成配置文件的路径（当前程序的路径）</li>
<li>如果配置文件存在，则直接读取，否则新建一个配置文件</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">// 1. 得到exe执行路径.  </div><div class="line">LPTSTR lpPath = new char[MAX_PATH];</div><div class="line">GetModuleFileName(NULL, lpPath, MAX_PATH);</div><div class="line">	</div><div class="line">// 2. C语言strrchr()函数：查找某字符在字符串中最后一次出现的位置</div><div class="line">char *pFind = strrchr(lpPath, &apos;\\&apos;);</div><div class="line">if (pFind == NULL)</div><div class="line">&#123;</div><div class="line">	return;</div><div class="line">&#125;</div><div class="line">*pFind = &apos;\0&apos;;</div><div class="line"></div><div class="line">// 3. 组合成配置文件的路径</div><div class="line">strcat(lpPath, &quot;\\config.ini&quot;);</div><div class="line"></div><div class="line">// 4. 如果配置文件存在，则直接读取，否则新建一个配置文件</div><div class="line">if (!PathFileExists(lpPath)) //写操作</div><div class="line">&#123;</div><div class="line">	string null_line = &quot;\n&quot;;</div><div class="line">	WritePrivateProfileString(&quot;Image&quot;, &quot;Width&quot;, to_string(image_width).c_str(), lpPath);</div><div class="line">	WritePrivateProfileString(&quot;Image&quot;, &quot;Height&quot;, to_string(image_height).c_str(), lpPath);</div><div class="line">	WritePrivateProfileString(&quot;Image&quot;, &quot;TH_is_edge&quot;, to_string(TH_is_edge).c_str(), lpPath);</div><div class="line">	WritePrivateProfileString(&quot;Image&quot;, &quot;Rect_scale&quot;, (to_string(Rect_scale) + &quot;\n&quot;).c_str(), lpPath);</div><div class="line">	.....</div><div class="line"></div><div class="line">&#125;</div><div class="line">else  //读操作</div><div class="line">&#123;</div><div class="line">	image_width = GetPrivateProfileInt(&quot;Image&quot;, &quot;Width&quot;, 0, lpPath);</div><div class="line">	image_height = GetPrivateProfileInt(&quot;Image&quot;, &quot;Height&quot;, 0, lpPath);</div><div class="line">	TH_is_edge = GetPrivateProfileInt(&quot;Image&quot;, &quot;TH_is_edge&quot;, 0, lpPath);</div><div class="line"></div><div class="line">	LPTSTR lpFloat = new char[8];</div><div class="line">	GetPrivateProfileString(&quot;Image&quot;, &quot;Rect_scale&quot;, &quot;&quot;, lpFloat, 8, lpPath);</div><div class="line">	Rect_scale = atof(lpFloat);</div><div class="line"></div><div class="line">	GetPrivateProfileString(&quot;Overlap_Threhold&quot;, &quot;TH_face_to_face_1&quot;, &quot;&quot;, lpFloat,8, lpPath);</div><div class="line">	TH_face_to_face_1 = atof(lpFloat);</div><div class="line">	.....</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="效果预览"><a href="#效果预览" class="headerlink" title="效果预览"></a>效果预览</h2><p><img src="/images/C_C++/ini_example.png" alt=""></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://en.wikipedia.org/wiki/INI_file" target="_blank" rel="external">Wikipedia: INI file</a></li>
<li><a href="http://blog.chinaunix.net/uid-24517893-id-2973290.html" target="_blank" rel="external">利用GetPrivateProfileString读取配置文件(.ini) </a></li>
<li><a href="http://www.cnblogs.com/it-duit/p/5632547.html" target="_blank" rel="external">C/C++ 关于如何读写ini配置文件 （小结）</a></li>
<li><a href="http://www.cnblogs.com/lishennan/p/5165136.html" target="_blank" rel="external">C++ 读取INI文件</a></li>
<li><a href="http://www.cnblogs.com/ZJoy/archive/2010/12/17/1909589.html" target="_blank" rel="external">C++读写ini配置文件GetPrivateProfileString()&amp;WritePrivateProfileString()</a></li>
<li><a href="http://c.biancheng.net/cpp/html/172.html" target="_blank" rel="external">C语言strrchr()函数：查找某字符在字符串中最后一次出现的位置</a></li>
</ul>
]]></content>
        
        <categories>
            
            <category> 编程语言 </category>
            
            <category> C_C++ </category>
            
            <category> 文件操作 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> ini配置文件 </tag>
            
            <tag> GetPrivateProfileString </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[解决C#程序多个进程同时打开同一个文件报错]]></title>
        <url>http://yoursite.com/2017/06/30/program-language-csharp/</url>
        <content type="html"><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>最近项目中遇到一个问题：开发的程序以只读的方式打开Navicat的历史文件<code>LogHistory.txt</code>（anyway 其他被多个程序读写的文件也一样），当打开多个程序后，关闭其中任何一个程序都会报错如下:</p>
<p><img src="/images/C_Sharp/C_Sharp_OpenFileError.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">错误信息：The process cannot access the file &quot;xxxx&quot;, because it is being used by another process</div></pre></td></tr></table></figure>
<h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>错误提示已经很明显，程序要读取文件的时候，发现已经被其他进程使用，读取失败而报错，在C#中，打开文件的时候，可以使用FileShare来完美解决这个问题:</p>
<p>代码修改前：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">string filepath = &quot;LogHistory.txt&quot;</div><div class="line">StreamReader sr = new StreamReader(filepath , System.Text.Encoding.Default);</div></pre></td></tr></table></figure>
<p>代码修改后：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">string filepath = &quot;LogHistory.txt&quot;</div><div class="line">FileStream fs = new FileStream(filepath, FileMode.Open, FileAccess.Read, FileShare.ReadWrite);//多一行</div><div class="line">StreamReader sr = new StreamReader(fs, System.Text.Encoding.Default);</div></pre></td></tr></table></figure>
<p>这里的共享方式 <code>FileShare.ReadWrite</code>，是指授予其它应用程序在操作该文件时所具有的权限为可读可写权限。</p>
<h1 id="知识拓展"><a href="#知识拓展" class="headerlink" title="知识拓展"></a>知识拓展</h1><p>解决多个进程同时读写文件，解决方案中采取共享方式FileShare，是指授予其它应用程序在操作该文件时所具有的权限，取值一般有None、Read、Write、ReadWrite，这里分别解释下：</p>
<ul>
<li>None：谢绝共享当前文件。文件关闭前，打开该文件的任何请求（由此进程或另一进程发出的请求）都将失败。 </li>
<li>Read：允许随后打开文件读取。如果未指定此标志，则文件关闭前，任何打开该文件以进行读取的请求（由此进程或另一进程发出的请求）都将失败。但是，即使指定了此标志，仍可能需要附加权限才能够访问该文件。</li>
<li>ReadWrite：允许随后打开文件读取或写入。如果未指定此标志，则文件关闭前，任何打开该文件以进行读取或写入的请求（由此进程或另一进程发出）都将失败。但是，即使指定了此标志，仍可能需要附加权限才能够访问该文件。 </li>
<li>Write：允许随后打开文件写入。如果未指定此标志，则文件关闭前，任何打开该文件以进行写入的请求（由此进程或另一进过程发出的请求）都将失败。但是，即使指定了此标志，仍可能需要附加权限才能够访问该文件。 </li>
</ul>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="http://blog.csdn.net/superhoy/article/details/7931234" target="_blank" rel="external">c# 读写文件时文件正由另一进程使用，因此该进程无法访问该文件</a></li>
<li><a href="http://www.cnblogs.com/wangjing215/p/4134978.html" target="_blank" rel="external">【转】C#读取文件时的共享方式</a></li>
<li><a href="http://www.cnblogs.com/hnsongbiao/p/4805382.html" target="_blank" rel="external">巧用FileShare解决C#读写文件时文件正由另一进程使用的bug</a></li>
</ul>
]]></content>
        
        <categories>
            
            <category> 编程语言 </category>
            
            <category> C_Sharp </category>
            
            <category> 文件操作 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> C# </tag>
            
            <tag> FileShare </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[使用Hexo发布静态博客到Github]]></title>
        <url>http://yoursite.com/2017/06/20/deploy-blog-to-github/</url>
        <content type="html"><![CDATA[<h1 id="使用Hexo发布静态博客到Github"><a href="#使用Hexo发布静态博客到Github" class="headerlink" title="使用Hexo发布静态博客到Github"></a>使用Hexo发布静态博客到Github</h1><h2 id="修改blog下的配置文件"><a href="#修改blog下的配置文件" class="headerlink" title="修改blog下的配置文件"></a>修改blog下的配置文件</h2><p><code>_config.yml</code> 文件的最后</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">deploy:</div><div class="line">  type: git</div><div class="line">  repository: git@github.com:hanchaow/hanchaow.github.io.git   //仓库所在目录</div><div class="line">  branch: master</div></pre></td></tr></table></figure>
<h2 id="使用git-bash发布博客"><a href="#使用git-bash发布博客" class="headerlink" title="使用git bash发布博客"></a>使用git bash发布博客</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">npm install hexo-deployer-git --save</div><div class="line">hexo d</div></pre></td></tr></table></figure>
<h2 id="遇到的问题汇总"><a href="#遇到的问题汇总" class="headerlink" title="遇到的问题汇总"></a>遇到的问题汇总</h2><p>执行命令<code>hexo d</code>提示<code>Deployer not found: github</code>，解决方法：<code>_config.yml</code> 文件中的deploy type为 <code>git</code> 而不是 <code>giuthub</code></p>
]]></content>
        
        <categories>
            
            <category> 博客 </category>
            
            <category> 发布博客 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[关于我]]></title>
        <url>http://yoursite.com/2017/06/20/about-me/</url>
        <content type="html"><![CDATA[<h1 id="自我介绍"><a href="#自我介绍" class="headerlink" title="自我介绍"></a>自我介绍</h1><ul>
<li><p>目前是尘世中一枚屌丝 <strong>程序猿</strong>，专注搬砖二十年（呃，作为一个90后，深感 <strong>“路漫漫其修远兮”</strong> 啊） ~~</p>
</li>
<li><p>求学与成长经历：</p>
<ul>
<li>1990年出生于河南的一个小村庄，刚好赶上了90后的第一趟班车，由于少年与火有缘，且算命先生说我五行缺，于是</li>
<li>2008年火急火燎地跑到了位于羊城的 <strong>“五山禅寺”</strong> 进修，鬼使神差学了计算专业，从此走上了码农的不归路…… </li>
<li>经过四年的 “禅悟”，同时坚持“药不能停”的原则和理念，2012年又厚着脸皮在位于霸都的 <strong>“南七技校”</strong>讨了一个硕士学位，主攻数据挖掘（不是蓝翔技校的挖掘机修理哈，^_^）</li>
<li>终于，2015年小硕毕业，由于学艺不精，加之父母期望一份稳定轻松的工作，所以校招选择了一家总部位于深圳的银行，进入该行的大零售团队工作，主做交易系统开发，看着高大上的名字，其实是非主流的开发技术……</li>
<li>工作一年有余，眼见深圳房价一天天水涨船高，想到一无所有的自己，想到技术一天天变渣，加之对银行业务不感兴趣，躁动不安之心涌起。所以，只好不停地问自己：这是我期望的生活吗？这是我追求的目标吗？</li>
<li>经过两个星期的认真思考和挣扎，总感觉自己的人生不应该如此四平八稳，毕竟人还是要有梦想的，<strong>一个人如果没有梦想，那和咸鱼有什么区别?</strong>  于是，2016年毅然决然辞职了，去了一家小创业公司打磨自己</li>
</ul>
</li>
</ul>
<ul>
<li>职业和研究兴趣：  <ul>
<li>公司目前主做人脸识别相关产品，自己负责算法的研发，所以主要的学习和研究领域有：<strong>神经网络与深度学习，机器学习，人工智能，数据挖掘，</strong>当然也正在学习一些<strong>传统CV</strong>的技术</li>
<li>至于编程语言，学过<strong>C/C++ 、Java</strong>，用过<strong>Pyhton、C#</strong> ，考虑工作需要和研究兴趣，准备加强下<strong>C++</strong> 和 <strong>Python</strong> 的学习</li>
</ul>
</li>
</ul>
<ul>
<li>自己的兴趣爱好：<ul>
<li>喜欢听音乐，尤其是经典的轻音乐：钢琴曲、古筝、葫芦丝、小提琴（注意，只是喜欢听哈）</li>
<li>喜欢看电影，尤其是经典励志大片，可以给自己正能量哦</li>
<li>喜欢瞎写字，所以有时候会小文艺一番~~</li>
<li>运动类的喜欢跑步，因为比较简单，还可以锻炼耐力和减肥</li>
<li>喜欢一个人静静地思考，虽然并不精通数学和算法，但是喜欢研究它们</li>
<li>喜欢Coding，更喜欢解决一个又一个遇到的问题和挑战，非常享受解决问题的过程，哪怕过程很痛苦</li>
</ul>
</li>
</ul>
<ul>
<li>对自己的评价：<ul>
<li>乐观开朗，踏实认真，不怕吃苦，敢于承担</li>
<li>喜欢干净利索，不喜欢拖泥带水</li>
<li>做事喜欢三思而后行，秉承“如果做，就把它做好”的理念</li>
<li>追求完美，但是考虑太多，容易陷入细节</li>
</ul>
</li>
</ul>
<ul>
<li>对来的未来期望：<ul>
<li>多读书，安安静静地 Reading, 去掉内心无事可做时的浮躁与不安, 力争做一个安静的美男子</li>
<li>多记录，记录和总结日常生活和工作中的<strong>所思、所想、所学</strong></li>
<li>多运动，保持健康的身体、充沛精力和良好的心态，身体永远是第一位的哟</li>
</ul>
</li>
</ul>
<ul>
<li>最喜欢的人生格言：<ul>
<li>Some thing to do, some one to love, some thing to hope.（有想做的事，有值得爱的人，有美丽的梦)</li>
</ul>
</li>
</ul>
<h1 id="我的项目"><a href="#我的项目" class="headerlink" title="我的项目"></a>我的项目</h1><ul>
<li>待补充</li>
</ul>
<h1 id="我的博客"><a href="#我的博客" class="headerlink" title="我的博客"></a>我的博客</h1><ul>
<li>吃水不忘打井人，感谢 Hexo工具和 <a href="https://github.com/GeekaholicLin" target="_blank" rel="external">林家兴 / Geekaholic 童鞋</a> 提供的 [精美主题 ylion] (<a href="https://github.com/GeekaholicLin/hexo-theme-ylion" target="_blank" rel="external">https://github.com/GeekaholicLin/hexo-theme-ylion</a>)</li>
<li>作为一枚打酱油的码农，生活中、工作中总会遇到各种各样的问题，一直想做点笔记和总结，却一直没实现，这次终于有了机会</li>
<li>深信<strong>好记性不如烂笔头</strong>，做笔记可以梳理自己想法，加深自己的理解，同时也可以用文字的形式分享自己的体会</li>
</ul>
<h1 id="我的联系方式"><a href="#我的联系方式" class="headerlink" title="我的联系方式"></a>我的联系方式</h1><ul>
<li><strong>GitHub:</strong>  <a href="https://github.com/hanchaow" target="_blank" rel="external">https://github.com/hanchaow</a></li>
<li><strong>Email:</strong> echo d2hjc2N1dEBnbWFpbC5jb20= | base64 -d</li>
<li><strong>Homepage:</strong> <a href="https://hanchaow.github.io/" target="_blank" rel="external">https://hanchaow.github.io/</a></li>
</ul>
]]></content>
        
        <categories>
            
            <category> 关于我 </category>
            
            <category> about_me </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
        <title><![CDATA[使用Hexo搭建静态博客]]></title>
        <url>http://yoursite.com/2017/06/20/CreaateBlogUsingHexo/</url>
        <content type="html"><![CDATA[<h1 id="使用Hexo搭建自己的博客"><a href="#使用Hexo搭建自己的博客" class="headerlink" title="使用Hexo搭建自己的博客"></a>使用Hexo搭建自己的博客</h1><h2 id="前言-amp-amp-致谢"><a href="#前言-amp-amp-致谢" class="headerlink" title="前言 &amp;&amp; 致谢"></a>前言 &amp;&amp; 致谢</h2><ul>
<li>感谢 <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a> : <code>A fast, simple &amp; powerful blog framework</code>(一个简单、快速、强大的博客发布工具，同时拥有众多的插件、优秀的主题，关键还支持Markdown，简直不能再酷)</li>
<li>感谢 Hexo提供的<a href="https://hexo.io/themes/" target="_blank" rel="external">精美主题</a></li>
<li>感谢 <a href="https://github.com/GeekaholicLin" target="_blank" rel="external">林家兴 / Geekaholic 童鞋</a> 提供的 <a href="https://github.com/GeekaholicLin/hexo-theme-ylion" target="_blank" rel="external">精美主题 ylion</a></li>
<li>搭建博客时，参考资料：<ul>
<li><a href="http://blog.liuxianan.com/build-blog-website-by-hexo-github.html" target="_blank" rel="external">使用hexo+github搭建免费个人博客详细教程</a></li>
<li><a href="http://www.tuicool.com/articles/ABFn2qU" target="_blank" rel="external">使用hexo搭建静态博客</a></li>
<li><a href="http://www.joryhe.com/2016-05-29-how_to_create_leancloud_read_Counter.html" target="_blank" rel="external">leanCloud,实现文章阅读量统计</a></li>
<li><a href="https://imsun.net/posts/gitment-introduction/" target="_blank" rel="external">Gitment：使用 GitHub Issues 搭建评论系统</a></li>
</ul>
</li>
</ul>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ul>
<li>安装 <a href="https://nodejs.org/en/" target="_blank" rel="external">node.js</a></li>
<li>Npm的 <a href="http://www.runoob.com/nodejs/nodejs-npm.html" target="_blank" rel="external">使用教程</a>,简单了解Npm的命名和使用方式</li>
<li>如果Npm安装模块的时候速度比较慢，推荐使用<a href="http://npm.taobao.org/" target="_blank" rel="external">淘宝的Npm镜像</a></li>
</ul>
<h2 id="Hexo的安装、初始化和启动"><a href="#Hexo的安装、初始化和启动" class="headerlink" title="Hexo的安装、初始化和启动"></a>Hexo的安装、初始化和启动</h2><h3 id="Hexo安装"><a href="#Hexo安装" class="headerlink" title="Hexo安装"></a>Hexo安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ npm install -g hexo</div></pre></td></tr></table></figure>
<h3 id="Hexo初始化一个博客"><a href="#Hexo初始化一个博客" class="headerlink" title="Hexo初始化一个博客"></a>Hexo初始化一个博客</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">d:</div><div class="line">cd Workplace</div><div class="line">mkdir hexo</div><div class="line">cd hexo</div><div class="line">hexo init blog   #初始化一个博客</div></pre></td></tr></table></figure>
<p><img src="/images/CreaateBlogUsingHexo/hexo_init.png" alt=""></p>
<h3 id="Hexo生成博客的静态页面，启动服务"><a href="#Hexo生成博客的静态页面，启动服务" class="headerlink" title="Hexo生成博客的静态页面，启动服务"></a>Hexo生成博客的静态页面，启动服务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ hexo g # 生成</div><div class="line">$ hexo s # 启动服务</div></pre></td></tr></table></figure>
<p><img src="/images/CreaateBlogUsingHexo/gen_public.png" alt=""></p>
<p>默认预览地址：<a href="http://localhost:4000" target="_blank" rel="external">http://localhost:4000</a>  （如果打不开，可能是端口冲突）</p>
<p>更快捷的命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo s -p 4000 #指定端口</div></pre></td></tr></table></figure>
<p><img src="/images/CreaateBlogUsingHexo/hexo_start.png" alt=""></p>
<h3 id="Hexo主题更换"><a href="#Hexo主题更换" class="headerlink" title="Hexo主题更换"></a>Hexo主题更换</h3><p>Hexo初始化一个博客后，会默认使用主题 landscape ，如果想更换其他主题，参考如下步骤：</p>
<p>这里以使用 <a href="https://github.com/GeekaholicLin/hexo-theme-ylion" target="_blank" rel="external">hexo-theme-ylion</a>为例</p>
<h4 id="下载主题"><a href="#下载主题" class="headerlink" title="下载主题"></a>下载主题</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd themes</div><div class="line">git clone git@github.com:GeekaholicLin/hexo-theme-ylion.git</div></pre></td></tr></table></figure>
<p>或者直接使用命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git clone git@github.com:GeekaholicLin/hexo-theme-ylion.git themes/ylion</div></pre></td></tr></table></figure>
<h4 id="安装依赖（如果有依赖的话，注意切换到你新建的blog目录）"><a href="#安装依赖（如果有依赖的话，注意切换到你新建的blog目录）" class="headerlink" title="安装依赖（如果有依赖的话，注意切换到你新建的blog目录）"></a>安装依赖（如果有依赖的话，注意切换到你新建的blog目录）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">npm install hexo-renderer-ejs --save</div><div class="line">npm install hexo-renderer-less --save</div><div class="line">npm install hexo-generator-feed --save</div><div class="line">npm install hexo-generator-sitemap --save</div><div class="line">npm install hexo-deployer-git --save</div><div class="line">npm install ejs@^1.0.0 --save</div><div class="line">npm install moment --save</div><div class="line">npm install utils-merge --save</div></pre></td></tr></table></figure>
<h4 id="修改-blog目录下的全局配置文件：-config-yml"><a href="#修改-blog目录下的全局配置文件：-config-yml" class="headerlink" title="修改 blog目录下的全局配置文件：_config.yml"></a>修改 blog目录下的全局配置文件：<code>_config.yml</code></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># Extensions</div><div class="line">## Plugins: https://hexo.io/plugins/</div><div class="line">## Themes: https://hexo.io/themes/</div><div class="line">theme: ylion #要替换的主题名称（主题的目录名称）</div></pre></td></tr></table></figure>
<h4 id="重新发布，测试效果："><a href="#重新发布，测试效果：" class="headerlink" title="重新发布，测试效果："></a>重新发布，测试效果：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo s -p 4000 #指定端口</div></pre></td></tr></table></figure>
<h2 id="小的建议"><a href="#小的建议" class="headerlink" title="小的建议"></a>小的建议</h2><h3 id="Hexo-常用命令汇总："><a href="#Hexo-常用命令汇总：" class="headerlink" title="Hexo 常用命令汇总："></a>Hexo 常用命令汇总：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hexo new &quot;postName&quot; #新建文章</div><div class="line">hexo new page &quot;pageName&quot; #新建页面</div><div class="line">hexo generate #生成静态页面至public目录</div><div class="line">hexo server #开启预览访问端口（默认端口4000，&apos;ctrl + c&apos;关闭server）</div><div class="line">hexo deploy #部署到GitHub</div><div class="line">hexo help  # 查看帮助</div><div class="line">hexo version  #查看Hexo的版本</div></pre></td></tr></table></figure>
<h3 id="简写命令："><a href="#简写命令：" class="headerlink" title="简写命令："></a>简写命令：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">hexo n == hexo new</div><div class="line">hexo g == hexo generate</div><div class="line">hexo s == hexo server</div><div class="line">hexo d == hexo deploy</div></pre></td></tr></table></figure>
<h3 id="组合命令："><a href="#组合命令：" class="headerlink" title="组合命令："></a>组合命令：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hexo s -g #生成并本地预览</div><div class="line">hexo d -g #生成并上传</div><div class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo s -p 4000 #清楚静态文件，生产文件并启动服务</div></pre></td></tr></table></figure>
<h3 id="hexo-插件推荐"><a href="#hexo-插件推荐" class="headerlink" title="hexo 插件推荐"></a>hexo 插件推荐</h3><p>如果你为了更好地写博客，推荐两个Hexo插件：hexo-browsersync 和 hexo-admin。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">npm install hexo-browsersync --save</div><div class="line">npm install hexo-admin --save</div></pre></td></tr></table></figure>
<p>hexo-browsersync 主要用于监听刷新，hexo-admin 提供类似后台的功能。前者无需配置，按照平常启用server预览即可。</p>
<h2 id="常见问题汇总："><a href="#常见问题汇总：" class="headerlink" title="常见问题汇总："></a>常见问题汇总：</h2><h3 id="插入picture的方法，采用绝对路径"><a href="#插入picture的方法，采用绝对路径" class="headerlink" title="插入picture的方法，采用绝对路径"></a>插入picture的方法，采用绝对路径</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">![](/images/CreaateBlogUsingHexo/pic_name.png)</div></pre></td></tr></table></figure>
<h3 id="Yaml解析报错"><a href="#Yaml解析报错" class="headerlink" title="Yaml解析报错"></a>Yaml解析报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">ERROR Process failed: _posts/CreaateBlogUsingHexo.md</div><div class="line">YAMLException: can not read a block mapping entry; a multiline key may not be an implicit key at line 10, column 1:</div><div class="line"></div><div class="line">    ^</div><div class="line">    at generateError (D:\Workplace\hexo\blog\node_modules\js-yaml\lib\js-yaml\loader.js:162:10)</div><div class="line">    at throwError (D:\Workplace\hexo\blog\node_modules\js-yaml\lib\js-yaml\loader.js:168:9)</div><div class="line">    at readBlockMapping (D:\Workplace\hexo\blog\node_modules\js-yaml\lib\js-yaml\loader.js:1045:9)</div><div class="line">    at composeNode (D:\Workplace\hexo\blog\node_modules\js-yaml\lib\js-yaml\loader.js:1331:12)</div><div class="line">    at readDocument (D:\Workplace\hexo\blog\node_modules\js-yaml\lib\js-yaml\loader.js:1493:3)</div><div class="line">    at loadDocuments (D:\Workplace\hexo\blog\node_modules\js-yaml\lib\js-yaml\loader.js:1549:5)</div><div class="line">    at Object.load (D:\Workplace\hexo\blog\node_modules\js-yaml\lib\js-yaml\loader.js:1566:19)</div><div class="line">    at parseYAML (D:\Workplace\hexo\blog\node_modules\hexo-front-matter\lib\front_matter.js:80:21)</div></pre></td></tr></table></figure>
<p><img src="/images/CreaateBlogUsingHexo/yml_error.png" alt=""></p>
<p>原因： YAML方式，以三短线开始和结束，每个类型后面加 <code>:</code>并且要加一个空格！！</p>
<h3 id="阅读量统计"><a href="#阅读量统计" class="headerlink" title="阅读量统计"></a>阅读量统计</h3><p>使用leanCloud实现文章阅读量统计，在官网注册完成后，新建class时，假设新建的class名称为 <code>Counter</code>,那么主题里面的配置文件<code>_config.yml</code>需要做如下配置方可生效：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">leancloud:</div><div class="line">  </div><div class="line">enable: true</div><div class="line">  </div><div class="line">className: &quot;Counter&quot;</div></pre></td></tr></table></figure>
<p>即保持ClassName一致</p>
<p>另外，主题的配置文件<code>_config.yml</code>中的 <code>http</code>修改为为<code>https</code>，因为现在<code>github</code>发布个人网站默认是<code>https</code>的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">## script cdn --脚本cdn</div><div class="line">## leancloud -- 官网:https://leancloud.cn/</div><div class="line">leancloud_src: https://cdn1.lncld.net/static/js/2.1.0/av-min.js  ## 注意是：https</div></pre></td></tr></table></figure>
]]></content>
        
        <categories>
            
            <category> 博客 </category>
            
            <category> 搭建博客 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
        
    </entry>
    
    
    
</search>
