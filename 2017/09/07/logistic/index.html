<!doctype html>
<html>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable"  content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no">
    
<meta name="google-site-verification" content="true" />

    
    <!--Simple SEO-->


<meta name="robots" content=all />
<meta name="google" content=all />
<meta name="googlebot" content=all />
<meta name="verify" content=all />
    <!--Title-->

<title>Logistic回归分析 | Hank&#39;s Blog</title>

<link rel="alternate" href="/atom.xml" title="Hank&#39;s Blog" type="application/atom+xml">


<link rel="icon" href="/favicon.ico">

    
<link rel="stylesheet" href="/css/base.css">
<link rel="stylesheet" href="/css/pages/post.css">
<link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/thirdParty/highlight/github.css">
<link rel="stylesheet" href="/css/pages/comments.css">
    <!--script-->


<script src="https://cdn1.lncld.net/static/js/2.1.0/av-min.js"></script>
<script>
  var appId = "IXPidoLQf2PnqkOu5RqFmNXR-gzGzoHsz";
  var appKey = "bsuJnn4sTl3lUl0euYPoPGSp";
  var region = "null";
  AV.init({
    appId: appId,
    appKey: appKey,
    region: region
  });
</script>


<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
  var labels = "blog,gitment";
  labels = labels.split(",");
  var gitment = new Gitment({
    id: window.location.pathname,
    owner: "hanchaow",
    repo: "hanchaow.github.io",
    oauth: {
      client_id: "6c58b9a9fbcf9c77ed63",
      client_secret: "badcc6a6c12115f4c0c5494ad97c8388370ea294"
    },
    perPage: "20",
    labels: labels
  });
</script>

    
<script>
    var _hmt = _hmt || [];
    (function() {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?true";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    
</head>

<body>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<style>
    header{ top: 71px; position: absolute!important;}
    #container{padding-top: 151px!important;}
</style>
<div style="position:fixed;z-index:9999;left:0;top:0;width:100%;height:70px;background-color:#e0e0e0;color:#396CA5;border-bottom:1px solid #cecece;text-align:center;line-height:70px;white-space: nowrap;overflow: hidden;text-overflow: ellipsis">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->

<div id="wrap">
    <header  style="position: absolute;" >
    <div id="site-meta">
        <a href="/" id="logo">
            <h1 class="title">Hank&#39;s Blog</h1>
        </a>
        
        <h2 class="subtitle">码农的藏经阁</h2>
        
    </div>
    <ul id="nav">
        
            <li><a href="/"><i class="fa fa-home"></i>首页</a></li>
        
            <li><a href="/2017/06/20/about-me/"><i class="fa fa-user"></i>关于我</a></li>
        
            <li><a href="/atom.xml"><i class="fa fa-rss"></i>RSS</a></li>
        
        <li id="search"><a href="javascript:void(0)"><i class="fa fa-search"></i>搜索</a></li>
    </ul>
</header>

    <div id="container">
        
<ul id="sidebar">
    
    
<li class="widget notification">
    <i class="fa fa-bell-o"></i>
    <div>
        
<p> 
Hank 的博客，尘世中一迷途小码农！<br />
研究兴趣：数据挖掘、机器学习、深度学习
</p>
    </div>
</li>

    
    
<li class="widget widget-normal category">
    <h3 class="fa fa-th widget-title">分类</h3>
    <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/关于我/"><i class="fa" aria-hidden="true">关于我</i></a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/关于我/about-me/"><i class="fa" aria-hidden="true">about_me</i></a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/博客/"><i class="fa" aria-hidden="true">博客</i></a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/博客/发布博客/"><i class="fa" aria-hidden="true">发布博客</i></a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/博客/搭建博客/"><i class="fa" aria-hidden="true">搭建博客</i></a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/"><i class="fa" aria-hidden="true">机器学习</i></a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/经典算法/"><i class="fa" aria-hidden="true">经典算法</i></a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link current" href="/categories/深度学习/"><i class="fa" aria-hidden="true">深度学习</i></a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link current" href="/categories/深度学习/Caffe/"><i class="fa" aria-hidden="true">Caffe</i></a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link current" href="/categories/深度学习/Caffe/LossLayers/"><i class="fa" aria-hidden="true">LossLayers</i></a><span class="category-list-count">1</span></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程语言/"><i class="fa" aria-hidden="true">编程语言</i></a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/编程语言/C-C/"><i class="fa" aria-hidden="true">C_C++</i></a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/编程语言/C-C/文件操作/"><i class="fa" aria-hidden="true">文件操作</i></a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程语言/C-Sharp/"><i class="fa" aria-hidden="true">C_Sharp</i></a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/编程语言/C-Sharp/文件操作/"><i class="fa" aria-hidden="true">文件操作</i></a><span class="category-list-count">1</span></li></ul></li></ul></li></ul>
</li>


    
    
<li class="widget widget-normal archive">
  <h3 class="fa fa-archive widget-title">归档</h3>
    <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/"><i class="fa" aria-hidden="true">十月 2017</i></a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/"><i class="fa" aria-hidden="true">九月 2017</i></a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/"><i class="fa" aria-hidden="true">六月 2017</i></a><span class="archive-list-count">5</span></li></ul>
</li>


    
    
<li class="widget widget-normal popular-posts" id="popular-posts">
    <h3 class="fa fa-thermometer-3 widget-title">热门文章</h3>
    <ul id="popular-content">
        <li class="load-first"><i class="fa fa-spinner fa-pulse"></i></li>
    </ul>
</li>

    
    
<li class="widget widget-normal tags">
  <h3 class="fa fa-tags widget-title">标签云</h3>
  <div class="tagcloud-content">
    
      <a href="/tags/Caffe/" style="font-size: 0.14rem; color: #69c">Caffe</a> <a href="/tags/Loss/" style="font-size: 0.14rem; color: #69c">Loss</a> <a href="/tags/Logistic/" style="font-size: 0.14rem; color: #69c">Logistic</a> <a href="/tags/回归分析/" style="font-size: 0.14rem; color: #69c">回归分析</a> <a href="/tags/Hexo/" style="font-size: 0.2rem; color: #0a407c">Hexo</a> <a href="/tags/C/" style="font-size: 0.14rem; color: #69c">C#</a> <a href="/tags/FileShare/" style="font-size: 0.14rem; color: #69c">FileShare</a> <a href="/tags/ini配置文件/" style="font-size: 0.14rem; color: #69c">ini配置文件</a> <a href="/tags/GetPrivateProfileString/" style="font-size: 0.14rem; color: #69c">GetPrivateProfileString</a>
  </div>
</li>

    
    
<li class="widget widget-normal friends-link">
    <h3 class="fa fa-globe widget-title">友链</h3><br/>

    
        <a href="http://www.reconova.com/company.shtml" class="fa" target="_blank">瑞为科技</a>

    
        <a href="http://dm.ustc.edu.cn/" class="fa" target="_blank">实验室</a>

    
        <a href="http://staff.ustc.edu.cn/~cheneh/" class="fa" target="_blank">导师</a>

    
        <a href="http://staff.ustc.edu.cn/~qiliuql/" class="fa" target="_blank">大师兄</a>

    

</li>

    
</ul>


        <div id="main">
    <article id="post">
        <div id="post-header">

            <h1 id="Logistic回归分析">
                
                Logistic回归分析
                
            </h1>
            <div class="article-meta">
    
    
    <span class="categories-meta fa-wrap">
            <i class="fa fa-folder-open-o"></i>
        <span>经典算法</span>
    </span>
    
    
    <span class="fa-wrap">
         <i class="fa fa-tags"></i>
        <span class="tags-meta">
            
            Logistic
            
        </span>
    </span>
    
    
    <span class="fa-wrap">
        <i class="fa fa-clock-o"></i>
        <span class="date-meta ">2017/09/07</span>
    </span>
    
    
    <span class="fa-wrap">
            <i class="fa fa-thermometer-three-quarters"></i>
        <span class="hits hits-meta " data-leadcloud-title="Logistic回归分析"
              data-leadcloud-url="/2017/09/07/logistic/"><i class="fa fa-spinner fa-spin"></i></span>
    </span>
    
    
</div>

            
            
        </div>
        
        <div id="post-body">
            <h1 id="Logistic回归分析"><a href="#Logistic回归分析" class="headerlink" title="Logistic回归分析"></a>Logistic回归分析</h1><p>$\qquad Logistic$回归为概率型非线性回归模型，机器学习常用的二类分类器，其表达式为：</p>
<script type="math/tex; mode=display">
\begin{align*}
&z=w_{1}*x_{1}+w_{2}*x_{2}+\cdots+w_{n}*x_{n}+b=\sum_{i=0}^n w_{i}x_{i}  （其中 b等于w_{0}，x_{0}等于1）\\
&f(x) = \frac{1}{1+exp(-z)}
\end{align*}</script><p>$\qquad$即对于二分类，如果$f(x)\ge{0.5}$,则$x$属于第一类，即预测$y=1$，反之$x$属于第二类，预测$y=0$；样本的分布如下，其中，$C_1$表示第一个类别，$C_2$表示第二个类别，样本个数为$n$：</p>
<script type="math/tex; mode=display">
\begin{align*}
&trainingData &x^1 \quad x^2 \quad x^3 \quad \cdots \quad x^n   \\
&labels &C_1 \quad C_1 \quad C_2 \quad \cdots \quad C_1
\end{align*}</script><p>$\qquad$我们的目的是：对于类别为$1$的正样本$f_{w,b}(x)$ 尽可能大,而类别为$2$的负样本$f_{w,b}(x)$ 尽可能小,则我们需要最大化：$L(w,b)=f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))\cdots\ f_{w,b}(x^n)$<br>来寻找最佳的$w$和$b$，即：</p>
<script type="math/tex; mode=display">
w^*,b^* = arg\max\limits_{w,b}(L(w,b))\Longrightarrow\ w^*,b^* = arg\min\limits_{w,b}(-ln{L(w,b)})</script><h1 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a><a href="https://zh.wikipedia.org/zh-hans/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95" target="_blank" rel="external">随机梯度下降法</a></h1><p>$\qquad$ 我们需要优化的函数：</p>
<script type="math/tex; mode=display">
\begin{align*}
-ln{L(w,b)} = -\{ln{f_{w,b}(x^1)}+lnf_{w,b}(x^2)+ln(1-f_{w,b}(x^3))+\cdots lnf_{w,b}(x^n)\}\quad \\
\end{align*}</script><script type="math/tex; mode=display">
\qquad 假设：
\begin{cases}
\hat{y} = 1 \qquad \qquad x\in1 \\\
\\
\hat{y} = 0 \qquad \qquad x\in0
\end{cases}
\qquad 已知\,f(x) = \frac{1}{1+exp(-z)}\quad z = \sum_{i=0}^n  w_{i}x_{i} \qquad 则</script><p>$\qquad$ 我们需要优化的函数简化为：</p>
<script type="math/tex; mode=display">
\begin{align*}
&ln{L(w,b)} =\sum_{j=1}^{n}\{\hat{y}^j\,lnf_{w,b}(x^j)+(1-\hat{y}^j)\,ln(1-f_{w,b}(x^j))\} \\
&当\,\,\hat{y}=1时\quad \hat{y}\,lnf_{w,b}(x)+(1-\hat y)\,ln(1-f_{w,b}(x)) = lnf_{w,b}(x)  \\
&当\,\,\hat{y}=0时\quad \hat{y}\,lnf_{w,b}(x)+(1-\hat y)\,ln(1-f_{w,b}(x)) = ln(1-f_{w,b}(x))
\end{align*}</script><p>$\qquad$ 即均满足上式 , 因此有:</p>
<script type="math/tex; mode=display">
\begin{align*}
\frac{\partial lnL(w,b)}{\partial w_i}=\sum_{j=1}^{n}\hat{y}^j\frac{ \partial lnf_{w,b}(x^j) }{\partial w_i}+(1-\hat{y}^j)\frac{\partial (1-lnf_{w,b}(x^j))}{\partial w_i} \\
\end{align*}</script><p>$\qquad$ 而：</p>
<script type="math/tex; mode=display">
\begin{align*}
\frac{\partial lnf_{w,b}(x)}{\partial w_i}
&=\frac{\partial lnf_{w,b}(x)}{\partial z}*\frac{\partial z}{\partial w_i} \\
&=\frac{1}{f_{w,b}(x)}* \frac{\partial f_{w,b}(x)}{\partial z}*x_i \\
&=\frac{1}{f_{w,b}(x)}*f_{w,b}(x)*(1-f_{w,b}(x))*x_i \\
&=(1-f_{w,b}(x))*x_i \\
\end{align*}</script><p>$\qquad$ 同理，有：</p>
<script type="math/tex; mode=display">\frac{\partial (1-lnf_{w,b}(x))}{\partial w_i}=f_{w,b}(x)*x_i</script><p>$\qquad$ 则化简后有：</p>
<script type="math/tex; mode=display">
\begin{align*}
\frac{\partial lnL(w,b)}{\partial w_i}
&=\sum_{j=1}^{n}\hat{y}^j\frac{ \partial lnf_{w,b}(x^j) }{\partial w_i}+(1-\hat{y}^j)\frac{\partial (1-lnf_{w,b}(x^j))}{\partial w_i} \\
&=\sum_{j=1}^{n}\{\hat{y}^j(1-f_{w,b}(x^j))x^j_i+(1-\hat{y}^j)*f_{w,b}(x^j)x^j_i\} \\
&= \sum_{j=1}^{n}(\hat{y}^j -f_{w,b}(x^j))x^j_i
\end{align*}</script><p>$\qquad b的推导与w的相似，可以得到w的更新迭代过程：w_{i} \leftarrow w_{i}-\alpha*\sum_{j=0}^{n}(\hat{y}^j-f_{w,b}(x^j))x^j_i$</p>
<p>$\qquad$简单总结对比下logistic回归和线性回归：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>$\quad \, Step$</th>
<th>$\qquad \qquad \qquad logistic \quad regression$</th>
<th>$\quad  \qquad  linear \quad regression$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\quad functions$</td>
<td>$\qquad \qquad\quad f_{w,b}(x) = \delta(\sum_{i=0}^{n}w_ix_i+b)$</td>
<td>$\qquad \quad f_{w,b}(x) = \sum_{i=0}^{n}w_ix_i+b$</td>
</tr>
<tr>
<td>$loss function$</td>
<td>$\qquad \qquad\qquad\hat{y}:1 \,\, for\,\, C_1,0\,\, for \,\, C_2 $    $\qquad \quad ln{L(w,b)} =\sum_{j=1}^{n}\{\hat{y}^j\,lnf_{w,b}(x^j)+(1-\hat{y}^j)\,ln(1-f_{w,b}(x^j))\}$</td>
<td>$\qquad\quad \hat{y} : a \quad real\quad number $ $\qquad \quad L(w,b) =\frac{1}{2} \sum_{j=1}^{n}(f_{w,b}(x^j)-\hat{y}^j)^2$</td>
</tr>
<tr>
<td>$optimize$</td>
<td>$\qquad \qquad\quad w_{i} \leftarrow w_{i}-\alpha*\sum_{j=0}^{n}(\hat{y}^j-f_{w,b}(x^j))x^j_i$</td>
<td>$\qquad\quad w_{i} \leftarrow w_{i}-\alpha*\sum_{j=0}^{n}(\hat{y}^j-f_{w,b}(x^j))x^j_i$</td>
</tr>
</tbody>
</table>
</div>
<h1 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h1><h2 id="为什么选用-crossEntropy-损失函数，而不用L2损失函数"><a href="#为什么选用-crossEntropy-损失函数，而不用L2损失函数" class="headerlink" title="为什么选用$crossEntropy$损失函数，而不用L2损失函数"></a>为什么选用$crossEntropy$损失函数，而不用L2损失函数</h2><p>$\qquad答:logistic不像linear \,\, regression使用L2损失函数的原因，主要是由于logistic的funcion的形式，由于sigmoid函数的存在，如果logistic采取L2 loss时，损失函数为：$</p>
<p>$\qquad \qquad \frac{\partial (f_{w,b}(x)-\hat{y})^2}{\partial w_i}=2(f_{w,b}(x)-\hat{y})f_{w,b}(x)(1-f_{w,b}(x))x_i$</p>
<p>$\qquad 则当\hat{y}=1, f_{w,b}(x) = 1 \quad 预测为1 ，即预测完全正确时 \quad loss=0 \quad$</p>
<p>$\qquad 但是当\hat{y}=1,f_{w,b}(x) = 0 \quad 预测为0 ，即预测完全错误时 \quad loss却依然为0 \quad显然不对$</p>
<h2 id="logistic-regression-的分类概率为什么选取了-sigmoid-函数"><a href="#logistic-regression-的分类概率为什么选取了-sigmoid-函数" class="headerlink" title="$logistic \,\,regression$的分类概率为什么选取了$sigmoid$函数"></a><a href="https://www.zhihu.com/question/54707359" target="_blank" rel="external">$logistic \,\,regression$的分类概率为什么选取了$sigmoid$函数</a></h2><p>$\qquad 答: 我们假设样本的分布服从二次高斯分布，即$</p>
<p>$\qquad f_{\mu,\Sigma}(x) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp\{-\frac{1}{2}(x-\mu)^T|\Sigma|^{-1}(x-\mu)\},其中\mu为均值，\Sigma为协方差矩阵$</p>
<p>$\qquad 输入为x，输出f_{\mu,\Sigma}(x)为样本x的概率密度，高斯分布的形状分布取决于均值\mu和协方差矩阵\Sigma,因此需要求取最佳的高斯分布来满足样本的分布$</p>
<script type="math/tex; mode=display">
\begin{align*}
&Maximum Likelihood : L(\mu,\Sigma) = f_{\mu,\Sigma}(x^1)f_{\mu,\Sigma}(x^2)f_{\mu,\Sigma}(x^3)\cdots\cdots\ f_{\mu,\Sigma}(x^{N}) \\
&\mu^*,\Sigma^* = arg\max\limits_{\mu,\Sigma}L(\mu,\Sigma) \\
&\mu^* = \frac{1}{N}\sum_{i=0}^{N}{x^i} \\
&\Sigma^* = \frac{1}{N}\sum_{i=0}^{N}{(x^i-\mu^*)(x^i-\mu^*)^T}
\end{align*}</script><p>$\qquad对于一个二分类，我们假设类别1的样本高斯分布的均值为\mu^1,类别2的样本的高斯分布均值为\mu^2,他们具有相同的协方差\Sigma$</p>
<script type="math/tex; mode=display">
\begin{align*}
&\mu^1 = \sum_{i=1}^{n_1} x_i\quad (x_i \in C_1) ,\qquad \mu^2 = \sum_{i=1}^{n_2} x_i\quad(x_i \in C_2) \\
&\Sigma^1 = \sum_{i=1}^{n_1}(x_i-u^1)(x_i-u^1)^T ,\qquad \Sigma^2 = \sum_{i=1}^{n_2}(x_i-u^2)(x_i-u^2)^T \\
&\Sigma=\frac{n_1}{n_1+n_2}\Sigma^1+\frac{n_1}{n_1+n_2}\Sigma^2
\end{align*}</script><p>$\qquad对于样本x，如果属于C_1则有：$</p>
<script type="math/tex; mode=display">
\begin{align*}
$P(C_{1}|x)
&= \frac{P(C_{1},x)}{P(x)} \\
&=\frac{P(x|C_{1})*P(C_{1})}{P(x|C_{1})*P(C_{1})+P(x|C_{2})*P(C_{2})} \\
&=\frac{1}{1+\frac{P(x|C_{2})*P(C_{2})}{P(x|C_{1})*P(C_{1})}} \\
&=\frac{1}{1+exp(-\alpha)}

\qquad 其中\,\, \alpha= \ln(\frac{P(x|C_{1})*P(C_{1})}{P(x|C_{2})*P(C_{2})})
\end{align*}</script><p>$\qquad将P(x|C_i)带入高斯分布的公式:$</p>
<script type="math/tex; mode=display">
\begin{align*}
&P(x|C_1) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp\{-\frac{1}{2}(x-\mu^1)^T|\Sigma|^{-1}(x-\mu^1)\} \\
&P(x|C_2) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp\{-\frac{1}{2}(x-\mu^2)^T|\Sigma|^{-1}(x-\mu^2)\} \\
&P(C_1)=\frac{n_1}{n_1+n_2}\\
&P(C_2)=\frac{n_2}{n_1+n_2} \\
\end{align*}</script><p>$\qquad其中\alpha, w, b 分别为:$</p>
<script type="math/tex; mode=display">
\begin{align*}
\alpha
&= lnP(x|C_1)-lnP(x|C_2)+ln\frac{P(C_1)}{P(C_2)} \\
&=-\frac{1}{2}(x-\mu^1)^T|\Sigma|^{-1}(x-\mu^1)-(-\frac{1}{2}(x-\mu^2)^T|\Sigma|^{-1}(x-\mu^2))+ln\frac{n_1}{n_2}\\
&=-\frac{1}{2}x^T(\Sigma)^{-1}x+(u^1)^T(\Sigma)^{-1}x-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}x^T(\Sigma)^{-1}x-(u^2)^T(\Sigma)^{-1}x+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\
&=(u^1-u^2)^T(\Sigma)^{-1}x-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2}\\
&= wx+b \\

\\
&w = (u^1-u^2)^T(\Sigma)^{-1} \quad \\
&b =-\frac{1}{2}(u^1)^T(\Sigma)^{-1}u^1+\frac{1}{2}(u^2)^T(\Sigma)^{-1}u^2+ln\frac{n_1}{n_2} \\
\end{align*}</script><p>$\qquad 因此可以得到对于满足猜想的二次高斯分布的datasets，生成模型的分类表达式与logistic是一致的$</p>
<h1 id="生成model与判别model对比"><a href="#生成model与判别model对比" class="headerlink" title="生成model与判别model对比"></a>生成model与判别model对比</h1><h2 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h2><p>$\qquad$ 基于现有的样本，对样本分布做了一个猜测（极大似然），因此当数据集较少，或者有噪声的时候，都能达到一个较好的结果(不过分依赖于实际样本),并且可以根据不同的概率model完成样本分布的gauss</p>
<h2 id="判别模型"><a href="#判别模型" class="headerlink" title="判别模型"></a>判别模型</h2><p>$\qquad$ 基于决策的方式（判别式），通过优化方法(sgd)寻找最优参数，对样本的依赖大，样本充足时，其效果一般比生成模型好(基于事实 not 基于猜测)</p>
<h1 id="小扩展"><a href="#小扩展" class="headerlink" title="小扩展"></a>小扩展</h1><h2 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h2><p>$\qquad$ 基于先验概率得出的每个类别的后验概率为softmax函数，即：</p>
<script type="math/tex; mode=display">
\begin{align*}
P(C_i|x)
&= \frac{P(x|C_i)P(C_i)}{\sum_{j=1}^{n}P(x|C_j)P(C_j)}\\
&= \frac{exp(a_k)}{\sum_{j=1}^{n}a_j}
\end{align*}</script><h2 id="待续"><a href="#待续" class="headerlink" title="待续"></a>待续</h2>
        </div>
        <div id="post-footer">
            <div class="avatar" >
                <img src="/images/author.jpg" alt="avatar"/>
                
                <a href="javascript:void(0)" class="high-song">high起来 &#128541;</a>
                
                
                <a href="javascript:void(0)" class="donate fa">赠我一杯 &#128536;</a>
                
            </div>
            <ul class="author-profile-section">
                <li>作者:
                    
                    <a href="/about.html">Hanchaow</a>
                </li>
                <li>发表日期: <span>2017-09-07  09:45:01</span></li>
                
                <li>最后编辑日期: <span>2017-09-09  07:31:33</span></li>
                
                <li class="post-category">
                    文章分类:
                    
                    <a href="/categories/机器学习/">机器学习</a>
                    
                    <a href="/categories/机器学习/经典算法/">经典算法</a>
                    
                </li>
                <li class="post-tags">
                    文章标签:
                    
                    <a href="/tags/Logistic/">Logistic</a>
                    
                    <a href="/tags/回归分析/">回归分析</a>
                    
                </li>
                
                <li> 版权声明: <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank">
知识共享署名-非商业性使用-禁止演绎 3.0 未本地化版本许可协议（CC BY-NC-ND 3.0）
</a></li>
                
            </ul>
            <div id="donate-wrap">
                
                
                
                <img src="/images/zhifubao.JPG" alt="支付宝付款" class="donate-img">
                
                <img src="/images/weixin.JPG" alt="微信付款" class="donate-img">
                
                
            </div>
        </div>
    </article>
    <div class="article-nav">
        
        <a href="/2017/10/20/Caffe_SoftmaxLossLayer/" class="pre-post fa fa-caret-left">Caffe之SoftmaxWithLossLayer原理推导和代码解析</a>
        
        
        <a href="/2017/06/30/program-language-c-cplus/" class="next-post fa">C/C++读写ini配置文件</a>
        
    </div>
    
    <div id="comments">
        

<script>
  gitment.render(document.getElementById("comments"));
</script>



    </div>
    
</div>


    </div>
    <footer id="footer">
    
    <div class="social">
        
        <a href="/" class="fa fa-free-code-camp" target="_blank" title="freecodecamp"></a>
        
        <a href="https://github.com/hanchaow" class="fa fa-github" target="_blank" title="Follow me~"></a>
        
        <a href="mailto:wanghanchao@reconova.com" class="fa fa-envelope" target="_blank" title="mail to me"></a>
        
        <a href="/2017/06/20/about-me/" class="fa fa-graduation-cap" target="_blank" title="关于我"></a>
        
    </div>
    
    <div>
        
        <a href="/" class="copyright-links">Hanchaow</a>&copy;2015 - 2017.All Rights
        Reserved.
    </div>
    <p>Powered by <a href="https://hexo.io" class="copyright-links" target="_blank">Hexo</a> | Theme by <a
                href="https://github.com/GeekaholicLin" class="copyright-links" target="_blank">GeekaholicLin</a>
    </p>
    
    
    <p>
        <span id="busuanzi_container_site_uv" class="fa fa-bar-chart">
        欢迎第<span id="busuanzi_value_site_uv"><i class="fa fa-spinner fa-spin"></i></span>位小伙伴~
        </span>
    </p>
    
</footer>

</div>
    <ul id="tools">
    <li class="totop-btn fa fa-angle-up"></li>
    
    <li class="toc-btn fa fa-list-ul"></li>
    
    

    
    <li class="comment-btn fa fa-comments-o">
        <a href="#comments" title="comments"></a>
    </li>
    
</ul>
<p id="process"></p>
<div id="search-overlay">
    <div class="search-area-wrap">
        <div id="search-area">
            <div class="input-wrap focus">
                <i class="fa fa-search" aria-hidden="true"></i>
                <input id="search-input" autofocus autocomplete="off" type="text"
                       placeholder="search this website..."/>
            </div>
            <ul id="search-result">
                <li class="load-first"><i class="fa fa-spinner fa-spin"></i></li>
            </ul>
        </div>
    </div>
</div>

    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Logistic回归分析"><span class="toc-number">1.</span> <span class="toc-text">Logistic回归分析</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#随机梯度下降法"><span class="toc-number">2.</span> <span class="toc-text">随机梯度下降法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#思考题"><span class="toc-number">3.</span> <span class="toc-text">思考题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#为什么选用-crossEntropy-损失函数，而不用L2损失函数"><span class="toc-number">3.1.</span> <span class="toc-text">为什么选用$crossEntropy$损失函数，而不用L2损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#logistic-regression-的分类概率为什么选取了-sigmoid-函数"><span class="toc-number">3.2.</span> <span class="toc-text">$logistic \,\,regression$的分类概率为什么选取了$sigmoid$函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#生成model与判别model对比"><span class="toc-number">4.</span> <span class="toc-text">生成model与判别model对比</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#生成模型"><span class="toc-number">4.1.</span> <span class="toc-text">生成模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#判别模型"><span class="toc-number">4.2.</span> <span class="toc-text">判别模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#小扩展"><span class="toc-number">5.</span> <span class="toc-text">小扩展</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#多分类"><span class="toc-number">5.1.</span> <span class="toc-text">多分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#待续"><span class="toc-number">5.2.</span> <span class="toc-text">待续</span></a></li></ol></li></ol>


    <script src="/js/highsong.js"></script>



<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });

</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script src="/js/search.js"></script>
<script type="text/javascript">
    //theme config datas
    var copyrightObj = {};
    copyrightObj.enable = 'true';
    copyrightObj.triggerCopyLength = '200';
    copyrightObj.appendText = '商业转载请联系作者获得授权,非商业转载请注明出处 © example';
    var leancloudObj = {};
    leancloudObj.enable = 'true';
    leancloudObj.className = 'Counter';
    leancloudObj.limits = '10';
</script>
<script type="text/javascript">
    var search = {};
    var search_path = "search.xml";
    if (!search_path) {
        search_path = "search.xml";
    }
    search.path = "/" + search_path;
    search.func =  _ajax.init();
</script>
<script src="/js/app.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>
</html>